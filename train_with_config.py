#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ChatGLM-6B QLoRAè®­ç»ƒè„šæœ¬
æ”¯æŒå¤šç§é¢„è®¾é…ç½®ï¼Œç”¨æˆ·å¯æ ¹æ®GPUæ˜¾å­˜é€‰æ‹©åˆé€‚çš„é…ç½®
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModel,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    set_seed
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from transformers import BitsAndBytesConfig

# æ·»åŠ configç›®å½•åˆ°Pythonè·¯å¾„
config_path = Path(__file__).parent / "config"
sys.path.insert(0, str(config_path))

from train_config_simple import (
    TrainingConfig,
    SmallGPUConfig,
    MediumGPUConfig,
    LargeGPUConfig,
    FastTrainingConfig
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

class TripletDataset(Dataset):
    """ä¸‰å…ƒç»„æ•°æ®é›†ç±»"""
    
    def __init__(self, tokenizer, data_file: str, max_source_length: int = 512, max_target_length: int = 256):
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.data = self.load_data(data_file)
        
    def load_data(self, data_file: str) -> List[Dict]:
        """åŠ è½½JSONLæ•°æ®"""
        data = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        logger.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®ä» {data_file}")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        instruction = item.get("instruction", "")
        input_text = item.get("input", "")
        output_text = item.get("output", "")
        
        # æ„å»ºå®Œæ•´çš„prompt
        if instruction and input_text:
            prompt = f"{instruction}\n{input_text}"
        elif instruction:
            prompt = instruction
        else:
            prompt = input_text
        
        # ç¼–ç è¾“å…¥
        source_encoding = self.tokenizer(
            prompt,
            max_length=self.max_source_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        # ç¼–ç è¾“å‡º
        target_encoding = self.tokenizer(
            output_text,
            max_length=self.max_target_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "input_ids": source_encoding["input_ids"].flatten(),
            "attention_mask": source_encoding["attention_mask"].flatten(),
            "labels": target_encoding["input_ids"].flatten()
        }

class ChatGLMTrainer:
    """ChatGLMè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
        
    def setup_model_and_tokenizer(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info("åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.MODEL_NAME,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        logger.info("è®¾ç½®é‡åŒ–é…ç½®...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=self.config.LOAD_IN_4BIT,
            bnb_4bit_compute_dtype=getattr(torch, self.config.BNB_4BIT_COMPUTE_DTYPE),
            bnb_4bit_use_double_quant=self.config.BNB_4BIT_USE_DOUBLE_QUANT,
            bnb_4bit_quant_type=self.config.BNB_4BIT_QUANT_TYPE
        )
        
        logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        self.model = AutoModel.from_pretrained(
            self.config.MODEL_NAME,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        # å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
        self.model = prepare_model_for_kbit_training(self.model)
        
        logger.info("è®¾ç½®LoRAé…ç½®...")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.config.LORA_R,
            lora_alpha=self.config.LORA_ALPHA,
            lora_dropout=self.config.LORA_DROPOUT,
            target_modules=self.config.LORA_TARGET_MODULES
        )
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        self.model.print_trainable_parameters()
        
    def prepare_datasets(self):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
        self.train_dataset = TripletDataset(
            self.tokenizer,
            self.config.TRAIN_FILE,
            self.config.MAX_SOURCE_LENGTH,
            self.config.MAX_TARGET_LENGTH
        )
        
        logger.info("å‡†å¤‡éªŒè¯æ•°æ®é›†...")
        self.eval_dataset = TripletDataset(
            self.tokenizer,
            self.config.VALIDATION_FILE,
            self.config.MAX_SOURCE_LENGTH,
            self.config.MAX_TARGET_LENGTH
        )
        
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("è®¾ç½®è®­ç»ƒå‚æ•°...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            overwrite_output_dir=True,
            num_train_epochs=self.config.NUM_EPOCHS,
            per_device_train_batch_size=self.config.BATCH_SIZE,
            per_device_eval_batch_size=self.config.EVAL_BATCH_SIZE,
            gradient_accumulation_steps=self.config.GRADIENT_ACCUMULATION_STEPS,
            learning_rate=self.config.LEARNING_RATE,
            warmup_steps=self.config.WARMUP_STEPS,
            logging_steps=self.config.LOGGING_STEPS,
            save_steps=self.config.SAVE_STEPS,
            eval_steps=self.config.EVAL_STEPS,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=self.config.LOAD_BEST_MODEL_AT_END,
            metric_for_best_model=self.config.METRIC_FOR_BEST_MODEL,
            greater_is_better=self.config.GREATER_IS_BETTER,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            fp16=self.config.FP16,
            gradient_checkpointing=self.config.GRADIENT_CHECKPOINTING,
            report_to=None,
            run_name="chatglm-6b-triplet-qlora"
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            label_pad_token_id=-100,
            pad_to_multiple_of=None,
            padding=True
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        logger.info("å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        logger.info("ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        self.tokenizer.save_pretrained(str(output_dir))
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir.absolute()}")
        
        return True

def detect_gpu_config():
    """è‡ªåŠ¨æ£€æµ‹GPUé…ç½®å¹¶æ¨èåˆé€‚çš„è®­ç»ƒé…ç½®"""
    if not torch.cuda.is_available():
        logger.warning("æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¸æ¨èï¼‰")
        return SmallGPUConfig()
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    logger.info(f"æ£€æµ‹åˆ°GPUå†…å­˜: {gpu_memory:.1f} GB")
    
    if gpu_memory < 12:
        logger.info("æ¨èä½¿ç”¨å°æ˜¾å­˜é…ç½®")
        return SmallGPUConfig()
    elif gpu_memory < 24:
        logger.info("æ¨èä½¿ç”¨ä¸­ç­‰æ˜¾å­˜é…ç½®")
        return MediumGPUConfig()
    else:
        logger.info("æ¨èä½¿ç”¨å¤§æ˜¾å­˜é…ç½®")
        return LargeGPUConfig()

def print_config_info(config):
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    logger.info("=" * 50)
    logger.info("è®­ç»ƒé…ç½®:")
    logger.info(f"  æ¨¡å‹: {config.MODEL_NAME}")
    logger.info(f"  è®­ç»ƒè½®æ•°: {config.NUM_EPOCHS}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE}")
    logger.info(f"  æ¢¯åº¦ç´¯ç§¯: {config.GRADIENT_ACCUMULATION_STEPS}")
    logger.info(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
    logger.info(f"  å­¦ä¹ ç‡: {config.LEARNING_RATE}")
    logger.info(f"  LoRA rank: {config.LORA_R}")
    logger.info(f"  LoRA alpha: {config.LORA_ALPHA}")
    logger.info(f"  è¾“å…¥é•¿åº¦: {config.MAX_SOURCE_LENGTH}")
    logger.info(f"  è¾“å‡ºé•¿åº¦: {config.MAX_TARGET_LENGTH}")
    logger.info(f"  è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
    
    # ä¼°ç®—æ˜¾å­˜ä½¿ç”¨
    estimated_memory = 5.5 + config.BATCH_SIZE * 0.5
    logger.info(f"  é¢„ä¼°æ˜¾å­˜éœ€æ±‚: ~{estimated_memory:.1f} GB")
    logger.info("=" * 50)

def check_data_files(config):
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    train_file = Path(config.TRAIN_FILE)
    val_file = Path(config.VALIDATION_FILE)
    
    if not train_file.exists():
        logger.error(f"è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        logger.info("è¯·å…ˆè¿è¡Œ: python data/generate_triplet_data.py")
        return False
    
    if not val_file.exists():
        logger.error(f"éªŒè¯æ–‡ä»¶ä¸å­˜åœ¨: {val_file}")
        logger.info("è¯·å…ˆè¿è¡Œ: python data/generate_triplet_data.py")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ChatGLM-6B QLoRA ä¸‰å…ƒç»„æŠ½å–è®­ç»ƒ - é…ç½®åŒ–ç‰ˆæœ¬")
    print("=" * 80)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description="ChatGLM-6B QLoRAè®­ç»ƒ")
    parser.add_argument("--config", type=str, default="auto", 
                       choices=["auto", "small", "medium", "large", "fast", "default"],
                       help="é€‰æ‹©é…ç½®ç±»å‹")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lora_r", type=int, help="LoRA rank")
    parser.add_argument("--learning_rate", type=float, help="å­¦ä¹ ç‡")
    parser.add_argument("--output_dir", type=str, help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # é€‰æ‹©é…ç½®
    if args.config == "auto":
        config = detect_gpu_config()
        logger.info("ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„é…ç½®")
    elif args.config == "small":
        config = SmallGPUConfig()
        logger.info("ä½¿ç”¨å°æ˜¾å­˜GPUé…ç½®")
    elif args.config == "medium":
        config = MediumGPUConfig()
        logger.info("ä½¿ç”¨ä¸­ç­‰æ˜¾å­˜GPUé…ç½®")
    elif args.config == "large":
        config = LargeGPUConfig()
        logger.info("ä½¿ç”¨å¤§æ˜¾å­˜GPUé…ç½®")
    elif args.config == "fast":
        config = FastTrainingConfig()
        logger.info("ä½¿ç”¨å¿«é€Ÿè®­ç»ƒé…ç½®")
    else:
        config = TrainingConfig()
        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.epochs:
        config.NUM_EPOCHS = args.epochs
    if args.batch_size:
        config.BATCH_SIZE = args.batch_size
        config.EVAL_BATCH_SIZE = args.batch_size
    if args.lora_r:
        config.LORA_R = args.lora_r
        config.LORA_ALPHA = args.lora_r * 4  # è‡ªåŠ¨è°ƒæ•´alpha
    if args.learning_rate:
        config.LEARNING_RATE = args.learning_rate
    if args.output_dir:
        config.OUTPUT_DIR = args.output_dir
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.SEED)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_config_info(config)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_data_files(config):
        return False
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ChatGLMTrainer(config)
        
        # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        trainer.setup_model_and_tokenizer()
        
        # å‡†å¤‡æ•°æ®é›†
        trainer.prepare_datasets()
        
        # å¼€å§‹è®­ç»ƒ
        success = trainer.train()
        
        if success:
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {Path(config.OUTPUT_DIR).absolute()}")
            print("\nğŸš€ æ¥ä¸‹æ¥å¯ä»¥:")
            print("   1. ä½¿ç”¨æ¨ç†è„šæœ¬æµ‹è¯•æ¨¡å‹: python inference_triplet.py")
            print("   2. è¯„ä¼°æ¨¡å‹æ€§èƒ½: python evaluate_triplet.py")
            print("   3. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: ls output/chatglm-6b-triplet-qlora/")
            print("=" * 80)
        
        return success
        
    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)