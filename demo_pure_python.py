#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çº¯Pythonæ¼”ç¤ºè„šæœ¬ - å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
æ— éœ€shellè„šæœ¬ï¼Œç›´æ¥åœ¨Pythonä¸­åŠ è½½å’Œä½¿ç”¨æ¨¡å‹
"""

import os
import json
from pathlib import Path
from typing import List, Tuple

def demo_without_torch():
    """ä¸ä¾èµ–torchçš„æ¼”ç¤º - å±•ç¤ºé¡¹ç›®ç»“æ„å’Œé…ç½®"""
    print("=" * 70)
    print("ChatGLM-6B QLoRA ä¸‰å…ƒç»„æŠ½å– - çº¯Pythonæ¼”ç¤º")
    print("=" * 70)
    
    print("\nğŸ“ é¡¹ç›®ç»“æ„:")
    project_files = [
        "train_pure_python.py - åŸºç¡€è®­ç»ƒè„šæœ¬",
        "train_with_config.py - é…ç½®åŒ–è®­ç»ƒè„šæœ¬", 
        "example_train.py - ç¤ºä¾‹è®­ç»ƒè„šæœ¬",
        "inference_triplet.py - æ¨ç†è„šæœ¬",
        "evaluate_triplet.py - è¯„ä¼°è„šæœ¬",
        "config/train_config_simple.py - è®­ç»ƒé…ç½®",
        "data/train_triplet.jsonl - è®­ç»ƒæ•°æ®",
        "data/val_triplet.jsonl - éªŒè¯æ•°æ®"
    ]
    
    for file_desc in project_files:
        filename = file_desc.split(" - ")[0]
        desc = file_desc.split(" - ")[1]
        if Path(filename).exists():
            print(f"  âœ… {filename} - {desc}")
        else:
            print(f"  âŒ {filename} - {desc} (ä¸å­˜åœ¨)")
    
    print("\nğŸ”§ å¯ç”¨çš„è®­ç»ƒé…ç½®:")
    configs = [
        ("small", "å°æ˜¾å­˜GPU (8-12GB)", "batch_size=1, lora_r=4"),
        ("medium", "ä¸­ç­‰æ˜¾å­˜GPU (12-24GB)", "batch_size=4, lora_r=8"),
        ("large", "å¤§æ˜¾å­˜GPU (24GB+)", "batch_size=8, lora_r=16"),
        ("fast", "å¿«é€Ÿæµ‹è¯•", "epochs=1, batch_size=2"),
        ("auto", "è‡ªåŠ¨æ£€æµ‹", "æ ¹æ®GPUè‡ªåŠ¨é€‰æ‹©")
    ]
    
    for config_name, desc, params in configs:
        print(f"  ğŸ¯ {config_name:6} - {desc:20} ({params})")
    
    print("\nğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
    train_file = Path("data/train_triplet.jsonl")
    val_file = Path("data/val_triplet.jsonl")
    
    if train_file.exists() and val_file.exists():
        with open(train_file, 'r', encoding='utf-8') as f:
            train_count = sum(1 for line in f if line.strip())
        
        with open(val_file, 'r', encoding='utf-8') as f:
            val_count = sum(1 for line in f if line.strip())
        
        print(f"  ğŸ“ˆ è®­ç»ƒæ ·æœ¬: {train_count} æ¡")
        print(f"  ğŸ“Š éªŒè¯æ ·æœ¬: {val_count} æ¡")
        
        # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
        print("\nğŸ“ è®­ç»ƒæ•°æ®ç¤ºä¾‹:")
        with open(train_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    break
                if line.strip():
                    data = json.loads(line.strip())
                    print(f"  æ ·æœ¬ {i+1}:")
                    print(f"    è¾“å…¥: {data.get('input', '')[:50]}...")
                    print(f"    è¾“å‡º: {data.get('output', '')[:50]}...")
    else:
        print("  âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè¯·è¿è¡Œ: cd data && python generate_triplet_data.py")
    
    print("\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
    usage_examples = [
        "åŸºç¡€è®­ç»ƒ: python train_pure_python.py",
        "è‡ªåŠ¨é…ç½®: python train_with_config.py --config auto",
        "å°æ˜¾å­˜GPU: python train_with_config.py --config small",
        "å¿«é€Ÿæµ‹è¯•: python train_with_config.py --config fast",
        "è‡ªå®šä¹‰å‚æ•°: python train_with_config.py --config medium --epochs 2 --batch_size 2",
        "äº¤äº’å¼è®­ç»ƒ: python example_train.py"
    ]
    
    for i, example in enumerate(usage_examples, 1):
        print(f"  {i}. {example}")
    
    print("\nğŸ’¡ è®­ç»ƒå»ºè®®:")
    tips = [
        "æ–°æ‰‹ç”¨æˆ·: ä½¿ç”¨ train_pure_python.py å¼€å§‹",
        "è¿›é˜¶ç”¨æˆ·: ä½¿ç”¨ train_with_config.py è¿›è¡Œè°ƒå‚",
        "æ˜¾å­˜ä¸è¶³: ä½¿ç”¨ --config small æˆ–å‡å° batch_size",
        "å¿«é€ŸéªŒè¯: ä½¿ç”¨ --config fast è¿›è¡Œæµ‹è¯•",
        "è‡ªåŠ¨é€‰æ‹©: ä½¿ç”¨ --config auto è®©ç¨‹åºè‡ªåŠ¨æ£€æµ‹GPU"
    ]
    
    for tip in tips:
        print(f"  ğŸ’¡ {tip}")

def demo_model_usage_code():
    """å±•ç¤ºæ¨¡å‹ä½¿ç”¨ä»£ç ç¤ºä¾‹"""
    print("\n" + "=" * 70)
    print("æ¨¡å‹ä½¿ç”¨ä»£ç ç¤ºä¾‹")
    print("=" * 70)
    
    print("\nğŸ” 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹:")
    print("""
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModel.from_pretrained(
    "THUDM/chatglm-6b", 
    trust_remote_code=True,
    device_map="auto"
)

# åŠ è½½LoRAé€‚é…å™¨
model = PeftModel.from_pretrained(
    base_model, 
    "output/chatglm-6b-triplet-qlora"
)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(
    "THUDM/chatglm-6b", 
    trust_remote_code=True
)
""")
    
    print("\nğŸ¯ 2. è¿›è¡Œä¸‰å…ƒç»„æŠ½å–:")
    print("""
def extract_triplets(text):
    prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–ä¸‰å…ƒç»„ï¼Œæ ¼å¼ä¸º(ä¸»ä½“, å…³ç³», å®¢ä½“):\\n{text}"
    
    response, history = model.chat(
        tokenizer, 
        prompt, 
        history=[]
    )
    
    return response

# ä½¿ç”¨ç¤ºä¾‹
text = "è‹¹æœå…¬å¸æ˜¯ä¸€å®¶ç¾å›½çš„ç§‘æŠ€å…¬å¸ï¼Œæ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·ã€‚"
result = extract_triplets(text)
print(result)
# è¾“å‡º: (è‹¹æœå…¬å¸, æ˜¯, ç§‘æŠ€å…¬å¸), (è‹¹æœå…¬å¸, æ€»éƒ¨ä½äº, åŠ åˆ©ç¦å°¼äºšå·)
""")
    
    print("\nğŸ“Š 3. æ‰¹é‡å¤„ç†:")
    print("""
def batch_extract(texts):
    results = []
    for text in texts:
        result = extract_triplets(text)
        results.append({
            'input': text,
            'output': result
        })
    return results

# æ‰¹é‡å¤„ç†ç¤ºä¾‹
texts = [
    "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ã€‚",
    "å¼ ä¸‰åœ¨æ¸…åå¤§å­¦å­¦ä¹ è®¡ç®—æœºç§‘å­¦ã€‚",
    "ç‰¹æ–¯æ‹‰å…¬å¸ç”Ÿäº§ç”µåŠ¨æ±½è½¦ã€‚"
]

results = batch_extract(texts)
for result in results:
    print(f"è¾“å…¥: {result['input']}")
    print(f"è¾“å‡º: {result['output']}")
    print("-" * 50)
""")

def demo_training_process():
    """å±•ç¤ºè®­ç»ƒè¿‡ç¨‹"""
    print("\n" + "=" * 70)
    print("è®­ç»ƒè¿‡ç¨‹æ¼”ç¤º")
    print("=" * 70)
    
    print("\nğŸ”„ è®­ç»ƒæµç¨‹:")
    steps = [
        "1. æ•°æ®å‡†å¤‡ - ç”Ÿæˆä¸‰å…ƒç»„æŠ½å–è®­ç»ƒæ•°æ®",
        "2. æ¨¡å‹åŠ è½½ - åŠ è½½ChatGLM-6BåŸºç¡€æ¨¡å‹",
        "3. é‡åŒ–é…ç½® - è®¾ç½®4bité‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜",
        "4. LoRAé…ç½® - è®¾ç½®ä½ç§©é€‚åº”å‚æ•°",
        "5. æ•°æ®å¤„ç† - å°†æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼",
        "6. è®­ç»ƒæ‰§è¡Œ - ä½¿ç”¨Trainerè¿›è¡Œè®­ç»ƒ",
        "7. æ¨¡å‹ä¿å­˜ - ä¿å­˜LoRAé€‚é…å™¨æƒé‡"
    ]
    
    for step in steps:
        print(f"  {step}")
    
    print("\nâš™ï¸ å…³é”®é…ç½®å‚æ•°:")
    params = [
        ("LoRA Rank", "8", "æ§åˆ¶é€‚é…å™¨å‚æ•°é‡ï¼Œè¶Šå¤§å®¹é‡è¶Šå¤§"),
        ("LoRA Alpha", "32", "ç¼©æ”¾å› å­ï¼Œé€šå¸¸ä¸ºrankçš„2-4å€"),
        ("Batch Size", "4", "æ‰¹æ¬¡å¤§å°ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´"),
        ("Learning Rate", "2e-4", "å­¦ä¹ ç‡ï¼ŒLoRAé€šå¸¸ç”¨è¾ƒå¤§å€¼"),
        ("Epochs", "3", "è®­ç»ƒè½®æ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆ"),
        ("Max Length", "512", "æœ€å¤§åºåˆ—é•¿åº¦")
    ]
    
    for param, value, desc in params:
        print(f"  {param:15}: {value:8} - {desc}")
    
    print("\nğŸ“ˆ è®­ç»ƒç›‘æ§:")
    monitoring = [
        "è®­ç»ƒæŸå¤± (Training Loss) - åº”è¯¥é€æ¸ä¸‹é™",
        "éªŒè¯æŸå¤± (Validation Loss) - ç”¨äºåˆ¤æ–­è¿‡æ‹Ÿåˆ",
        "å­¦ä¹ ç‡è°ƒåº¦ - é¢„çƒ­åé€æ¸è¡°å‡",
        "æ˜¾å­˜ä½¿ç”¨ - ç›‘æ§æ˜¯å¦è¶…å‡ºé™åˆ¶",
        "è®­ç»ƒé€Ÿåº¦ - æ¯æ­¥è®­ç»ƒæ—¶é—´"
    ]
    
    for item in monitoring:
        print(f"  ğŸ“Š {item}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    demo_without_torch()
    demo_model_usage_code()
    demo_training_process()
    
    print("\n" + "=" * 70)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    print("2. ç”Ÿæˆæ•°æ®: cd data && python generate_triplet_data.py")
    print("3. å¼€å§‹è®­ç»ƒ: python train_with_config.py --config auto")
    print("4. æµ‹è¯•æ¨¡å‹: python inference_triplet.py")
    print("\nè¯¦ç»†æ–‡æ¡£: PYTHON_TRAINING_GUIDE.md")
    print("å¿«é€Ÿå¼€å§‹: QUICK_START.md")
    print("=" * 70)

if __name__ == "__main__":
    main()