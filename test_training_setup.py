#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•è®­ç»ƒç¯å¢ƒè®¾ç½®
éªŒè¯æ˜¯å¦å¯ä»¥å¼€å§‹è®­ç»ƒ
"""

import os
import sys
import torch

def test_training_imports():
    """æµ‹è¯•è®­ç»ƒæ‰€éœ€çš„æ¨¡å—å¯¼å…¥"""
    print("ğŸš€ æµ‹è¯•è®­ç»ƒæ¨¡å—å¯¼å…¥")
    print("=" * 50)
    
    try:
        # æµ‹è¯•åŸºç¡€æ¨¡å—
        from glm_config import ProjectConfig
        print("âœ… é…ç½®æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•transformersæ¨¡å—
        from transformers import (
            AutoTokenizer, 
            AutoModel, 
            TrainingArguments, 
            Trainer,
            DataCollatorForSeq2Seq
        )
        print("âœ… Transformersæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•PEFTæ¨¡å—
        from peft import (
            get_peft_model, 
            LoraConfig, 
            TaskType,
            PeftModel
        )
        print("âœ… PEFTæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®å¤„ç†æ¨¡å—
        import json
        from datasets import Dataset
        print("âœ… æ•°æ®å¤„ç†æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\nğŸ”§ æµ‹è¯•é…ç½®åŠ è½½")
    print("-" * 30)
    
    try:
        from glm_config import ProjectConfig
        pc = ProjectConfig()
        
        print(f"ğŸ“‹ æ¨¡å‹è·¯å¾„: {pc.pre_model}")
        print(f"ğŸ“‹ è®­ç»ƒæ•°æ®: {pc.train_path}")
        print(f"ğŸ“‹ éªŒè¯æ•°æ®: {pc.dev_path}")
        print(f"ğŸ“‹ è¾“å‡ºç›®å½•: {pc.save_dir}")
        print(f"ğŸ“‹ æ‰¹æ¬¡å¤§å°: {pc.batch_size}")
        print(f"ğŸ“‹ æ¢¯åº¦ç´¯ç§¯: {pc.gradient_accumulation_steps}")
        print(f"ğŸ“‹ å­¦ä¹ ç‡: {pc.learning_rate}")
        print(f"ğŸ“‹ è®­ç»ƒè½®æ•°: {pc.epochs}")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        if os.path.exists(pc.train_path):
            print("âœ… è®­ç»ƒæ•°æ®æ–‡ä»¶å­˜åœ¨")
        else:
            print("âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
        if os.path.exists(pc.dev_path):
            print("âœ… éªŒè¯æ•°æ®æ–‡ä»¶å­˜åœ¨")
        else:
            print("âŒ éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•
        os.makedirs(pc.save_dir, exist_ok=True)
        print("âœ… è¾“å‡ºç›®å½•å·²å‡†å¤‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½")
    print("-" * 30)
    
    try:
        from glm_config import ProjectConfig
        pc = ProjectConfig()
        
        # è¯»å–è®­ç»ƒæ•°æ®
        with open(pc.train_path, 'r', encoding='utf-8') as f:
            train_lines = f.readlines()
        
        print(f"ğŸ“ˆ è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_lines)}")
        
        # è¯»å–éªŒè¯æ•°æ®
        with open(pc.dev_path, 'r', encoding='utf-8') as f:
            dev_lines = f.readlines()
        
        print(f"ğŸ“Š éªŒè¯æ ·æœ¬æ•°é‡: {len(dev_lines)}")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        import json
        sample = json.loads(train_lines[0])
        
        if 'context' in sample and 'target' in sample:
            print("âœ… æ•°æ®æ ¼å¼æ­£ç¡®")
            print(f"ğŸ“ æ ·æœ¬ç¤ºä¾‹:")
            print(f"   è¾“å…¥: {sample['context'][:50]}...")
            print(f"   è¾“å‡º: {sample['target'][:50]}...")
        else:
            print("âŒ æ•°æ®æ ¼å¼é”™è¯¯")
            print(f"   å®é™…å­—æ®µ: {list(sample.keys())}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False

def test_device_setup():
    """æµ‹è¯•è®¾å¤‡è®¾ç½®"""
    print("\nğŸ–¥ï¸ æµ‹è¯•è®¾å¤‡è®¾ç½®")
    print("-" * 30)
    
    try:
        print(f"ğŸ“‹ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"ğŸ“‹ CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"ğŸ“‹ CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"ğŸ“‹ GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"ğŸ“‹ GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®¾å¤‡æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_model_config_only():
    """ä»…æµ‹è¯•æ¨¡å‹é…ç½®ï¼ˆä¸åŠ è½½æƒé‡ï¼‰"""
    print("\nğŸ¤– æµ‹è¯•æ¨¡å‹é…ç½®")
    print("-" * 30)
    
    try:
        from transformers import AutoConfig
        from glm_config import ProjectConfig
        
        pc = ProjectConfig()
        
        print("ğŸ”„ åŠ è½½æ¨¡å‹é…ç½®...")
        config = AutoConfig.from_pretrained(
            pc.pre_model, 
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
        print(f"ğŸ“Š éšè—å±‚å¤§å°: {config.hidden_size}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_training_imports),
        ("é…ç½®åŠ è½½", test_config_loading),
        ("æ•°æ®åŠ è½½", test_data_loading),
        ("è®¾å¤‡è®¾ç½®", test_device_setup),
        ("æ¨¡å‹é…ç½®", test_model_config_only),
    ]
    
    success_count = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        if test_func():
            print(f"âœ… {test_name} é€šè¿‡")
            success_count += 1
        else:
            print(f"âŒ {test_name} å¤±è´¥")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("ğŸ‰ è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆï¼")
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ python train.py å¼€å§‹è®­ç»ƒ")
        print("2. ä½¿ç”¨ nvidia-smi ç›‘æ§GPUä½¿ç”¨æƒ…å†µ")
        print("3. è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥ä½¿ç”¨ Ctrl+C å®‰å…¨åœæ­¢")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·è§£å†³é—®é¢˜åå†å¼€å§‹è®­ç»ƒ")
        return 1

if __name__ == "__main__":
    sys.exit(main())