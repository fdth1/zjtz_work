#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹è·¯å¾„é…ç½®åŠ©æ‰‹
å¸®åŠ©ç”¨æˆ·å¿«é€Ÿé…ç½®æ­£ç¡®çš„ChatGLM-6Bæ¨¡å‹è·¯å¾„
"""

import os
import sys
import json
import shutil
from pathlib import Path

def find_chatglm_models():
    """æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯èƒ½çš„ChatGLM-6Bæ¨¡å‹è·¯å¾„"""
    possible_paths = [
        # ModelScopeç¼“å­˜è·¯å¾„
        "/root/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B",
        "/home/user/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B",
        os.path.expanduser("~/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B"),
        
        # Hugging Faceç¼“å­˜è·¯å¾„
        "/root/.cache/huggingface/hub/models--THUDM--chatglm-6b",
        "/home/user/.cache/huggingface/hub/models--THUDM--chatglm-6b",
        os.path.expanduser("~/.cache/huggingface/hub/models--THUDM--chatglm-6b"),
        
        # å¸¸è§çš„æœ¬åœ°è·¯å¾„
        "/models/ChatGLM-6B",
        "/data/models/ChatGLM-6B",
        "/workspace/models/ChatGLM-6B",
        "./models/ChatGLM-6B",
        "../models/ChatGLM-6B",
    ]
    
    found_models = []
    
    for path in possible_paths:
        if os.path.exists(path):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
            config_file = os.path.join(path, "config.json")
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ChatGLMæ¨¡å‹
                    if "chatglm" in config.get("model_type", "").lower() or \
                       "chatglm" in config.get("name_or_path", "").lower():
                        found_models.append(path)
                except:
                    pass
    
    return found_models

def validate_model_path(path):
    """éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦æœ‰æ•ˆ"""
    if not os.path.exists(path):
        return False, "è·¯å¾„ä¸å­˜åœ¨"
    
    required_files = ["config.json"]
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(os.path.join(path, file)):
            missing_files.append(file)
    
    if missing_files:
        return False, f"ç¼ºå°‘æ–‡ä»¶: {missing_files}"
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
    model_files = [
        "pytorch_model.bin",
        "pytorch_model-00001-of-00002.bin",  # åˆ†ç‰‡æ¨¡å‹
        "model.safetensors"
    ]
    
    has_model_file = any(
        os.path.exists(os.path.join(path, f)) for f in model_files
    )
    
    if not has_model_file:
        return False, "æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶"
    
    return True, "æ¨¡å‹è·¯å¾„æœ‰æ•ˆ"

def update_config_file(model_path):
    """æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„"""
    config_file = "glm_config.py"
    
    if not os.path.exists(config_file):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    # è¯»å–é…ç½®æ–‡ä»¶
    with open(config_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾å¹¶æ›¿æ¢æ¨¡å‹è·¯å¾„
    import re
    
    # åŒ¹é… self.pre_model = '...' æˆ– self.pre_model = "..."
    pattern = r"(self\.pre_model\s*=\s*['\"])[^'\"]*(['\"])"
    
    if re.search(pattern, content):
        new_content = re.sub(pattern, f"\\g<1>{model_path}\\g<2>", content)
        
        # å¤‡ä»½åŸæ–‡ä»¶
        backup_file = f"{config_file}.backup"
        shutil.copy2(config_file, backup_file)
        print(f"ğŸ“‹ å·²å¤‡ä»½åŸé…ç½®æ–‡ä»¶åˆ°: {backup_file}")
        
        # å†™å…¥æ–°é…ç½®
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"âœ… å·²æ›´æ–°é…ç½®æ–‡ä»¶ï¼Œæ¨¡å‹è·¯å¾„è®¾ç½®ä¸º: {model_path}")
        return True
    else:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹è·¯å¾„é…ç½®é¡¹")
        return False

def interactive_setup():
    """äº¤äº’å¼è®¾ç½®æ¨¡å‹è·¯å¾„"""
    print("ğŸ” æ­£åœ¨æœç´¢ç³»ç»Ÿä¸­çš„ChatGLM-6Bæ¨¡å‹...")
    
    found_models = find_chatglm_models()
    
    if found_models:
        print(f"\nâœ… æ‰¾åˆ° {len(found_models)} ä¸ªå¯èƒ½çš„æ¨¡å‹è·¯å¾„:")
        for i, path in enumerate(found_models, 1):
            valid, msg = validate_model_path(path)
            status = "âœ…" if valid else "âŒ"
            print(f"  {i}. {status} {path}")
            if not valid:
                print(f"     {msg}")
        
        print(f"  {len(found_models) + 1}. æ‰‹åŠ¨è¾“å…¥è·¯å¾„")
        print(f"  {len(found_models) + 2}. ä½¿ç”¨åœ¨çº¿æ¨¡å‹ (THUDM/chatglm-6b)")
        
        while True:
            try:
                choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹è·¯å¾„ (1-{len(found_models) + 2}): ").strip()
                choice_num = int(choice)
                
                if 1 <= choice_num <= len(found_models):
                    selected_path = found_models[choice_num - 1]
                    valid, msg = validate_model_path(selected_path)
                    if valid:
                        return selected_path
                    else:
                        print(f"âŒ é€‰æ‹©çš„è·¯å¾„æ— æ•ˆ: {msg}")
                        continue
                elif choice_num == len(found_models) + 1:
                    # æ‰‹åŠ¨è¾“å…¥
                    manual_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
                    if manual_path:
                        valid, msg = validate_model_path(manual_path)
                        if valid:
                            return manual_path
                        else:
                            print(f"âŒ è¾“å…¥çš„è·¯å¾„æ— æ•ˆ: {msg}")
                            continue
                elif choice_num == len(found_models) + 2:
                    # ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                    return "THUDM/chatglm-6b"
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©")
                    continue
                    
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                continue
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return None
    else:
        print("\nâŒ æœªæ‰¾åˆ°æœ¬åœ°ChatGLM-6Bæ¨¡å‹")
        print("\nğŸ’¡ æ‚¨å¯ä»¥:")
        print("1. æ‰‹åŠ¨è¾“å…¥æ¨¡å‹è·¯å¾„")
        print("2. ä½¿ç”¨åœ¨çº¿æ¨¡å‹ (éœ€è¦ç½‘ç»œä¸‹è½½)")
        print("3. é€€å‡ºå¹¶æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
        
        while True:
            choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
            
            if choice == "1":
                manual_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
                if manual_path:
                    valid, msg = validate_model_path(manual_path)
                    if valid:
                        return manual_path
                    else:
                        print(f"âŒ è¾“å…¥çš„è·¯å¾„æ— æ•ˆ: {msg}")
                        continue
            elif choice == "2":
                return "THUDM/chatglm-6b"
            elif choice == "3":
                return None
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©")
                continue

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ChatGLM-6B æ¨¡å‹è·¯å¾„é…ç½®åŠ©æ‰‹")
    print("=" * 50)
    
    # æ£€æŸ¥å½“å‰é…ç½®
    try:
        from glm_config import ProjectConfig
        pc = ProjectConfig()
        current_path = pc.pre_model
        print(f"ğŸ“‹ å½“å‰é…ç½®çš„æ¨¡å‹è·¯å¾„: {current_path}")
        
        if current_path != "THUDM/chatglm-6b":
            valid, msg = validate_model_path(current_path)
            if valid:
                print("âœ… å½“å‰æ¨¡å‹è·¯å¾„æœ‰æ•ˆ")
                
                choice = input("\næ˜¯å¦è¦é‡æ–°é…ç½®æ¨¡å‹è·¯å¾„? (y/N): ").strip().lower()
                if choice not in ['y', 'yes']:
                    print("ğŸ‘‹ ä¿æŒå½“å‰é…ç½®")
                    return 0
            else:
                print(f"âŒ å½“å‰æ¨¡å‹è·¯å¾„æ— æ•ˆ: {msg}")
        else:
            print("ğŸ“¡ å½“å‰ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
            
    except Exception as e:
        print(f"âš ï¸ è¯»å–å½“å‰é…ç½®æ—¶å‡ºé”™: {e}")
    
    # äº¤äº’å¼è®¾ç½®
    selected_path = interactive_setup()
    
    if selected_path is None:
        print("\nğŸ‘‹ é…ç½®å·²å–æ¶ˆ")
        return 1
    
    # æ›´æ–°é…ç½®æ–‡ä»¶
    print(f"\nğŸ”§ æ­£åœ¨æ›´æ–°é…ç½®æ–‡ä»¶...")
    if update_config_file(selected_path):
        print("\nâœ… æ¨¡å‹è·¯å¾„é…ç½®å®Œæˆï¼")
        
        # éªŒè¯æ–°é…ç½®
        print("\nğŸ§ª éªŒè¯æ–°é…ç½®...")
        try:
            # é‡æ–°å¯¼å…¥é…ç½®
            import importlib
            import glm_config
            importlib.reload(glm_config)
            
            pc = glm_config.ProjectConfig()
            print(f"ğŸ“‹ æ–°çš„æ¨¡å‹è·¯å¾„: {pc.pre_model}")
            
            if pc.pre_model != "THUDM/chatglm-6b":
                valid, msg = validate_model_path(pc.pre_model)
                if valid:
                    print("âœ… æ–°é…ç½®éªŒè¯é€šè¿‡")
                else:
                    print(f"âŒ æ–°é…ç½®éªŒè¯å¤±è´¥: {msg}")
                    return 1
            else:
                print("ğŸ“¡ å°†ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
            
        except Exception as e:
            print(f"âš ï¸ éªŒè¯æ–°é…ç½®æ—¶å‡ºé”™: {e}")
        
        print("\nğŸ‰ é…ç½®å®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒäº†:")
        print("   python check_model_path.py  # æ£€æŸ¥æ¨¡å‹è·¯å¾„")
        print("   python test_basic.py        # è¿è¡ŒåŸºç¡€æµ‹è¯•")
        print("   ./start_training.sh         # å¼€å§‹è®­ç»ƒ")
        
        return 0
    else:
        print("\nâŒ é…ç½®æ›´æ–°å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())