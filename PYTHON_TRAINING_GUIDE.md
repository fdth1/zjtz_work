# ChatGLM-6B QLoRA çº¯Pythonè®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨çº¯Pythonè„šæœ¬è¿›è¡ŒChatGLM-6Bçš„QLoRAå¾®è°ƒï¼Œæ— éœ€shellè„šæœ¬ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ç”Ÿæˆè®­ç»ƒæ•°æ®
cd data && python generate_triplet_data.py && cd ..
```

### 2. é€‰æ‹©è®­ç»ƒæ–¹å¼

#### æ–¹å¼1: åŸºç¡€è®­ç»ƒ (æ¨èæ–°æ‰‹)
```python
python train_pure_python.py
```

#### æ–¹å¼2: é…ç½®åŒ–è®­ç»ƒ (æ¨èè¿›é˜¶ç”¨æˆ·)
```python
# è‡ªåŠ¨æ£€æµ‹GPUé…ç½®
python train_with_config.py --config auto

# æ‰‹åŠ¨é€‰æ‹©é…ç½®
python train_with_config.py --config medium --epochs 2 --batch_size 4
```

#### æ–¹å¼3: ç¤ºä¾‹è®­ç»ƒ (æ¨èå­¦ä¹ )
```python
python example_train.py
```

## ğŸ“‹ è®­ç»ƒè„šæœ¬è¯´æ˜

### 1. `train_pure_python.py` - åŸºç¡€è®­ç»ƒè„šæœ¬
- **ç‰¹ç‚¹**: ç®€å•ç›´æ¥ï¼Œå‚æ•°å›ºå®š
- **é€‚ç”¨**: æ–°æ‰‹ç”¨æˆ·ï¼Œå¿«é€Ÿå¼€å§‹
- **é…ç½®**: å†…ç½®é»˜è®¤é…ç½®ï¼Œæ— éœ€ä¿®æ”¹

```python
# ä¸»è¦é…ç½®
model_name = "THUDM/chatglm-6b"
batch_size = 4
epochs = 3
lora_r = 8
learning_rate = 2e-4
```

### 2. `train_with_config.py` - é…ç½®åŒ–è®­ç»ƒè„šæœ¬
- **ç‰¹ç‚¹**: çµæ´»é…ç½®ï¼Œå¤šç§é¢„è®¾
- **é€‚ç”¨**: è¿›é˜¶ç”¨æˆ·ï¼Œéœ€è¦è°ƒå‚
- **é…ç½®**: æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶

```bash
# ä½¿ç”¨é¢„è®¾é…ç½®
python train_with_config.py --config small    # å°æ˜¾å­˜GPU (12GBä»¥ä¸‹)
python train_with_config.py --config medium   # ä¸­ç­‰æ˜¾å­˜GPU (12-24GB)
python train_with_config.py --config large    # å¤§æ˜¾å­˜GPU (24GB+)
python train_with_config.py --config fast     # å¿«é€Ÿæµ‹è¯•

# è‡ªå®šä¹‰å‚æ•°
python train_with_config.py --config medium --epochs 5 --batch_size 2 --lora_r 16
```

### 3. `example_train.py` - ç¤ºä¾‹è®­ç»ƒè„šæœ¬
- **ç‰¹ç‚¹**: äº¤äº’å¼é€‰æ‹©ï¼Œå¤šç§ç¤ºä¾‹
- **é€‚ç”¨**: å­¦ä¹ ç”¨æˆ·ï¼Œäº†è§£ä¸åŒé…ç½®
- **é…ç½®**: åŒ…å«4ç§ä¸åŒçš„è®­ç»ƒç¤ºä¾‹

## âš™ï¸ é…ç½®è¯¦è§£

### GPUæ˜¾å­˜é…ç½®å»ºè®®

| GPUæ˜¾å­˜ | æ¨èé…ç½® | æ‰¹æ¬¡å¤§å° | LoRA Rank | é¢„ä¼°è®­ç»ƒæ—¶é—´ |
|---------|----------|----------|-----------|--------------|
| 8-12GB  | small    | 1-2      | 4-8       | 4-6å°æ—¶      |
| 12-24GB | medium   | 4-8      | 8-16      | 2-3å°æ—¶      |
| 24GB+   | large    | 8-16     | 16-32     | 1-2å°æ—¶      |

### é…ç½®æ–‡ä»¶ `config/train_config_simple.py`

```python
# åŸºç¡€é…ç½®ç±»
class TrainingConfig:
    MODEL_NAME = "THUDM/chatglm-6b"
    NUM_EPOCHS = 3
    BATCH_SIZE = 4
    LORA_R = 8
    LEARNING_RATE = 2e-4
    # ... æ›´å¤šé…ç½®

# é¢„è®¾é…ç½®
class SmallGPUConfig(TrainingConfig):
    BATCH_SIZE = 1
    LORA_R = 4
    # é€‚åˆå°æ˜¾å­˜GPU

class MediumGPUConfig(TrainingConfig):
    BATCH_SIZE = 4
    LORA_R = 8
    # é€‚åˆä¸­ç­‰æ˜¾å­˜GPU
```

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### 1. ä¿®æ”¹é…ç½®æ–‡ä»¶
ç¼–è¾‘ `config/train_config_simple.py`:

```python
class MyCustomConfig(TrainingConfig):
    NUM_EPOCHS = 5          # è®­ç»ƒè½®æ•°
    BATCH_SIZE = 2          # æ‰¹æ¬¡å¤§å°
    LORA_R = 16            # LoRA rank
    LEARNING_RATE = 1e-4   # å­¦ä¹ ç‡
    OUTPUT_DIR = "output/my-model"  # è¾“å‡ºç›®å½•
```

### 2. ç¼–ç¨‹å¼é…ç½®
åœ¨Pythonä»£ç ä¸­ç›´æ¥é…ç½®:

```python
from config.train_config_simple import TrainingConfig
from train_with_config import ChatGLMTrainer

# åˆ›å»ºé…ç½®
config = TrainingConfig()
config.NUM_EPOCHS = 2
config.BATCH_SIZE = 1
config.LORA_R = 4

# å¼€å§‹è®­ç»ƒ
trainer = ChatGLMTrainer(config)
trainer.setup_model_and_tokenizer()
trainer.prepare_datasets()
trainer.train()
```

### 3. å‘½ä»¤è¡Œé…ç½®
ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é»˜è®¤é…ç½®:

```bash
python train_with_config.py \
    --config medium \
    --epochs 5 \
    --batch_size 2 \
    --lora_r 16 \
    --learning_rate 1e-4 \
    --output_dir output/my-custom-model
```

## ğŸ“Š è®­ç»ƒç›‘æ§

### 1. è®­ç»ƒæ—¥å¿—
è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—:
```
INFO - åŠ è½½åˆ†è¯å™¨...
INFO - è®¾ç½®é‡åŒ–é…ç½®...
INFO - åŠ è½½åŸºç¡€æ¨¡å‹...
INFO - è®¾ç½®LoRAé…ç½®...
trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.03
INFO - å‡†å¤‡è®­ç»ƒæ•°æ®é›†...
INFO - åŠ è½½äº† 903 æ¡æ•°æ®ä» data/train_triplet.jsonl
INFO - å‡†å¤‡éªŒè¯æ•°æ®é›†...
INFO - åŠ è½½äº† 101 æ¡æ•°æ®ä» data/val_triplet.jsonl
INFO - å¼€å§‹è®­ç»ƒ...
```

### 2. è®­ç»ƒè¿›åº¦
```
{'loss': 2.1234, 'learning_rate': 0.0002, 'epoch': 0.1}
{'eval_loss': 1.8765, 'eval_runtime': 12.34, 'epoch': 0.5}
```

### 3. æ¨¡å‹ä¿å­˜
è®­ç»ƒå®Œæˆåæ¨¡å‹ä¿å­˜åœ¨æŒ‡å®šç›®å½•:
```
output/chatglm-6b-triplet-qlora/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.bin
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ tokenizer.model
â””â”€â”€ special_tokens_map.json
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. æ˜¾å­˜ä¸è¶³ (CUDA out of memory)
**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å°æ‰¹æ¬¡å¤§å°
python train_with_config.py --config small --batch_size 1

# æˆ–ä½¿ç”¨æ›´å°çš„LoRAå‚æ•°
python train_with_config.py --lora_r 4
```

### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½®é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
git lfs clone https://huggingface.co/THUDM/chatglm-6b
```

### 3. æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨
**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç”Ÿæˆè®­ç»ƒæ•°æ®
cd data && python generate_triplet_data.py
```

### 4. ä¾èµ–åŒ…ç¼ºå¤±
**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# æˆ–å•ç‹¬å®‰è£…
pip install torch transformers peft bitsandbytes datasets accelerate
```

## ğŸ¯ è®­ç»ƒæŠ€å·§

### 1. å‚æ•°è°ƒä¼˜å»ºè®®
- **LoRA Rank**: ä»å°å¼€å§‹ (4â†’8â†’16)ï¼Œè§‚å¯Ÿæ•ˆæœ
- **å­¦ä¹ ç‡**: 2e-4 æ˜¯å¥½çš„èµ·ç‚¹ï¼Œå¯å°è¯• 1e-4 æˆ– 5e-4
- **æ‰¹æ¬¡å¤§å°**: æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å° â‰¥ 8
- **è®­ç»ƒè½®æ•°**: 3-5è½®é€šå¸¸è¶³å¤Ÿï¼Œé¿å…è¿‡æ‹Ÿåˆ

### 2. æ˜¾å­˜ä¼˜åŒ–
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (å·²é»˜è®¤å¼€å¯)
- å‡å°åºåˆ—é•¿åº¦
- ä½¿ç”¨æ›´å°çš„LoRAå‚æ•°
- å¯ç”¨4bité‡åŒ– (å·²é»˜è®¤å¼€å¯)

### 3. è®­ç»ƒåŠ é€Ÿ
- ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
- å‡å°‘è¯„ä¼°é¢‘ç‡
- ä½¿ç”¨FP16æ··åˆç²¾åº¦ (å·²é»˜è®¤å¼€å¯)

## ğŸ“ˆ è¿›é˜¶ç”¨æ³•

### 1. å¤šGPUè®­ç»ƒ
```bash
# ä½¿ç”¨torchrunè¿›è¡Œå¤šGPUè®­ç»ƒ
torchrun --nproc_per_node=2 train_with_config.py --config large
```

### 2. æ–­ç‚¹ç»­è®­
```python
# åœ¨é…ç½®ä¸­è®¾ç½®resume_from_checkpoint
config.resume_from_checkpoint = "output/chatglm-6b-triplet-qlora/checkpoint-500"
```

### 3. è‡ªå®šä¹‰æ•°æ®é›†
```python
# ä¿®æ”¹æ•°æ®æ–‡ä»¶è·¯å¾„
config.TRAIN_FILE = "path/to/your/train.jsonl"
config.VALIDATION_FILE = "path/to/your/val.jsonl"
```

## ğŸ” éªŒè¯è®­ç»ƒç»“æœ

### 1. å¿«é€Ÿæµ‹è¯•
```python
python inference_triplet.py
```

### 2. æ‰¹é‡è¯„ä¼°
```python
python evaluate_triplet.py
```

### 3. äº¤äº’å¼æµ‹è¯•
```python
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel

# åŠ è½½æ¨¡å‹
base_model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = PeftModel.from_pretrained(base_model, "output/chatglm-6b-triplet-qlora")
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)

# æµ‹è¯•æ¨ç†
text = "è‹¹æœå…¬å¸æ˜¯ä¸€å®¶ç¾å›½çš„ç§‘æŠ€å…¬å¸ã€‚"
prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–ä¸‰å…ƒç»„ï¼Œæ ¼å¼ä¸º(ä¸»ä½“, å…³ç³», å®¢ä½“):\n{text}"
response, history = model.chat(tokenizer, prompt, history=[])
print(response)
```

---

**æ€»ç»“**: æœ¬æŒ‡å—æä¾›äº†å¤šç§çº¯Pythonè®­ç»ƒæ–¹å¼ï¼Œä»ç®€å•çš„ä¸€é”®è®­ç»ƒåˆ°é«˜åº¦è‡ªå®šä¹‰çš„é…ç½®è®­ç»ƒï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„éœ€æ±‚ã€‚å»ºè®®æ–°æ‰‹ä» `train_pure_python.py` å¼€å§‹ï¼Œç†Ÿæ‚‰åä½¿ç”¨ `train_with_config.py` è¿›è¡Œæ›´ç²¾ç»†çš„è°ƒå‚ã€‚