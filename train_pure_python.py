#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChatGLM-6B QLoRAä¸‰å…ƒç»„æŠ½å–è®­ç»ƒ - çº¯Pythonå®ç°
ç›´æ¥åœ¨Pythonä¸­è¿›è¡ŒLoRAå¾®è°ƒï¼Œæ— éœ€shellè„šæœ¬æˆ–subprocessè°ƒç”¨
"""

import os
import json
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModel,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    set_seed
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
import bitsandbytes as bnb
from transformers import BitsAndBytesConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    # æ¨¡å‹é…ç½®
    model_name_or_path: str = "THUDM/chatglm-6b"
    
    # æ•°æ®é…ç½®
    train_file: str = "data/train_triplet.jsonl"
    validation_file: str = "data/val_triplet.jsonl"
    max_source_length: int = 512
    max_target_length: int = 256
    
    # è®­ç»ƒé…ç½®
    output_dir: str = "output/chatglm-6b-triplet-qlora"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
    warmup_steps: int = 100
    logging_steps: int = 10
    save_steps: int = 500
    eval_steps: int = 500
    
    # LoRAé…ç½®
    lora_r: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = None
    
    # é‡åŒ–é…ç½®
    load_in_4bit: bool = True
    bnb_4bit_compute_dtype: str = "float16"
    bnb_4bit_use_double_quant: bool = True
    bnb_4bit_quant_type: str = "nf4"
    
    def __post_init__(self):
        if self.lora_target_modules is None:
            self.lora_target_modules = ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]

class TripletDataset(Dataset):
    """ä¸‰å…ƒç»„æ•°æ®é›†ç±»"""
    
    def __init__(self, tokenizer, data_file: str, max_source_length: int = 512, max_target_length: int = 256):
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.data = self.load_data(data_file)
        
    def load_data(self, data_file: str) -> List[Dict]:
        """åŠ è½½JSONLæ•°æ®"""
        data = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        logger.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®ä» {data_file}")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        instruction = item.get("instruction", "")
        input_text = item.get("input", "")
        output_text = item.get("output", "")
        
        # æ„å»ºå®Œæ•´çš„prompt
        if instruction and input_text:
            prompt = f"{instruction}\n{input_text}"
        elif instruction:
            prompt = instruction
        else:
            prompt = input_text
        
        # ç¼–ç è¾“å…¥
        source_encoding = self.tokenizer(
            prompt,
            max_length=self.max_source_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        # ç¼–ç è¾“å‡º
        target_encoding = self.tokenizer(
            output_text,
            max_length=self.max_target_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "input_ids": source_encoding["input_ids"].flatten(),
            "attention_mask": source_encoding["attention_mask"].flatten(),
            "labels": target_encoding["input_ids"].flatten()
        }

class ChatGLMTrainer:
    """ChatGLMè®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
        
    def setup_model_and_tokenizer(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info("åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name_or_path,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        logger.info("è®¾ç½®é‡åŒ–é…ç½®...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=self.config.load_in_4bit,
            bnb_4bit_compute_dtype=getattr(torch, self.config.bnb_4bit_compute_dtype),
            bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant,
            bnb_4bit_quant_type=self.config.bnb_4bit_quant_type
        )
        
        logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        self.model = AutoModel.from_pretrained(
            self.config.model_name_or_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        # å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
        self.model = prepare_model_for_kbit_training(self.model)
        
        logger.info("è®¾ç½®LoRAé…ç½®...")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules
        )
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        self.model.print_trainable_parameters()
        
    def prepare_datasets(self):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
        self.train_dataset = TripletDataset(
            self.tokenizer,
            self.config.train_file,
            self.config.max_source_length,
            self.config.max_target_length
        )
        
        logger.info("å‡†å¤‡éªŒè¯æ•°æ®é›†...")
        self.eval_dataset = TripletDataset(
            self.tokenizer,
            self.config.validation_file,
            self.config.max_source_length,
            self.config.max_target_length
        )
        
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("è®¾ç½®è®­ç»ƒå‚æ•°...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            overwrite_output_dir=True,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            eval_steps=self.config.eval_steps,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            fp16=True,
            gradient_checkpointing=True,
            report_to=None,
            run_name="chatglm-6b-triplet-qlora"
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            label_pad_token_id=-100,
            pad_to_multiple_of=None,
            padding=True
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        logger.info("å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        logger.info("ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        self.tokenizer.save_pretrained(str(output_dir))
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir.absolute()}")
        
        return True

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    logger.info("æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        logger.info(f"CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¸æ¨èï¼‰")
    
    # æ£€æŸ¥å†…å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"GPUå†…å­˜: {gpu_memory:.1f} GB")
        if gpu_memory < 12:
            logger.warning("GPUå†…å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨24GB+æ˜¾å­˜çš„GPU")
    
    return True

def check_data_files(config: TrainingConfig):
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    train_file = Path(config.train_file)
    val_file = Path(config.validation_file)
    
    if not train_file.exists():
        logger.error(f"è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        logger.info("è¯·å…ˆè¿è¡Œ: python data/generate_triplet_data.py")
        return False
    
    if not val_file.exists():
        logger.error(f"éªŒè¯æ–‡ä»¶ä¸å­˜åœ¨: {val_file}")
        logger.info("è¯·å…ˆè¿è¡Œ: python data/generate_triplet_data.py")
        return False
    
    # æ£€æŸ¥æ–‡ä»¶å†…å®¹
    try:
        with open(train_file, 'r', encoding='utf-8') as f:
            train_count = sum(1 for line in f if line.strip())
        
        with open(val_file, 'r', encoding='utf-8') as f:
            val_count = sum(1 for line in f if line.strip())
        
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {train_count}")
        logger.info(f"éªŒè¯æ ·æœ¬æ•°: {val_count}")
        
        if train_count == 0 or val_count == 0:
            logger.error("æ•°æ®æ–‡ä»¶ä¸ºç©º")
            return False
            
    except Exception as e:
        logger.error(f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ChatGLM-6B QLoRA ä¸‰å…ƒç»„æŠ½å–æ¨¡å‹è®­ç»ƒ - çº¯Pythonå®ç°")
    print("=" * 70)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig()
    
    # æ‰“å°é…ç½®
    logger.info("è®­ç»ƒé…ç½®:")
    for key, value in config.__dict__.items():
        if not key.startswith('_'):
            logger.info(f"  {key}: {value}")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        return False
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_data_files(config):
        return False
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ChatGLMTrainer(config)
        
        # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        trainer.setup_model_and_tokenizer()
        
        # å‡†å¤‡æ•°æ®é›†
        trainer.prepare_datasets()
        
        # å¼€å§‹è®­ç»ƒ
        success = trainer.train()
        
        if success:
            print("\n" + "=" * 70)
            print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {Path(config.output_dir).absolute()}")
            print("ğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†:")
            print("   python inference_triplet.py")
            print("=" * 70)
        
        return success
        
    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)