import os
import time
import copy
import argparse
import gc
from functools import partial
import torch
import peft
# autocastæ˜¯PyTorchä¸­ä¸€ç§æ··åˆç²¾åº¦çš„æŠ€æœ¯ï¼Œå¯åœ¨ä¿æŒæ•°å€¼ç²¾åº¦çš„æƒ…å†µä¸‹æé«˜è®­ç»ƒé€Ÿåº¦å’Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚
from torch.cuda.amp import GradScaler
from transformers import AutoTokenizer, AutoConfig, AutoModel, get_scheduler, AutoModelForCausalLM
from utils.common_utils import *
from data_handle.data_loader import *
from glm_config import *


# å…¼å®¹ä¸åŒ PyTorch ç‰ˆæœ¬çš„ autocast APIï¼ˆ2.x ä½¿ç”¨ torch.amp.autocast("cuda")ï¼Œæ—©æœŸç‰ˆæœ¬ä½¿ç”¨ torch.cuda.amp.autocastï¼‰
try:
    from torch.amp import autocast as _amp_autocast  # type: ignore
    def autocast_cuda():
        return _amp_autocast("cuda")
except Exception:  # pragma: no cover
    from torch.cuda.amp import autocast as _cuda_autocast  # type: ignore
    def autocast_cuda():
        return _cuda_autocast()

pc = ProjectConfig()

def print_gpu_memory():
    """æ‰“å°GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        print(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
    
def clear_gpu_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def evaluate_model(model, dev_dataloader):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å½“å‰æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚

    Args:
        model: å½“å‰æ¨¡å‹
        data_loader: æµ‹è¯•é›†çš„dataloader
    """
    model.eval()
    loss_list = []
    with torch.no_grad():
        for batch in dev_dataloader:
            if pc.use_lora:
                with autocast_cuda():
                    loss = model(
                        input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),
                        labels=batch['labels'].to(dtype=torch.long, device=pc.device)
                    ).loss
            else:
                loss = model(
                    input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),
                    labels=batch['labels'].to(dtype=torch.long, device=pc.device)
                ).loss
            loss_list.append(float(loss.cpu().detach()))
    model.train()
    return sum(loss_list) / len(loss_list)


def model2train():
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒé…ç½®...")
    print_gpu_memory()
    
    # åˆå§‹åŒ–tokenizer
    print("ğŸ“ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(pc.pre_model, trust_remote_code=True)

    # åˆå§‹åŒ–æ¨¡å‹é…ç½®
    print("âš™ï¸ åŠ è½½æ¨¡å‹é…ç½®...")
    config = AutoConfig.from_pretrained(pc.pre_model, trust_remote_code=True)

    if pc.use_ptuning:
        config.pre_seq_len = pc.pre_seq_len
        config.prefix_projection = pc.prefix_projection
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½ChatGLM-6Bæ¨¡å‹...")
    model = AutoModel.from_pretrained(
        pc.pre_model,
        config=config,
        trust_remote_code=True,
        torch_dtype=torch.float16 if pc.fp16 else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )

    print("ğŸ”§ é…ç½®æ¨¡å‹ä¼˜åŒ–è®¾ç½®...")
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()
    # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœæ˜¾å­˜
    model.config.use_cache = False

    if pc.use_ptuning:
        if pc.fp16:
            model.transformer.prefix_encoder.half()
        else:
            model.transformer.prefix_encoder.float()
    
    print(f'ğŸ¯ æ¨¡å‹è¾“å‡ºå±‚: {model.lm_head}')
    
    if pc.use_lora:
        print("ğŸ”— é…ç½®LoRA...")
        model.lm_head = CastOutputToFloat(model.lm_head)
        peft_config = peft.LoraConfig(
            task_type=peft.TaskType.CAUSAL_LM,
            inference_mode=False,
            r=pc.lora_rank,
            lora_alpha=pc.lora_alpha,
            lora_dropout=pc.lora_dropout,
            target_modules=["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]  # ChatGLM-6Bçš„ç›®æ ‡æ¨¡å—
        )
        model = peft.get_peft_model(model, peft_config)

    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    if not hasattr(model, 'device') or model.device != torch.device(pc.device):
        model = model.to(pc.device)
    
    print('ğŸ“Š æ¨¡å‹è®­ç»ƒå‚æ•°ç»Ÿè®¡:')
    if hasattr(model, 'print_trainable_parameters'):
        model.print_trainable_parameters()
    
    print_gpu_memory()

    # é…ç½®ä¼˜åŒ–å™¨
    print("ğŸ¯ é…ç½®ä¼˜åŒ–å™¨...")
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": pc.weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=pc.learning_rate)
    
    # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if pc.fp16 and torch.cuda.is_available() else None
    
    # åŠ è½½æ•°æ®
    print("ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataloader, dev_dataloader = get_data()
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = len(train_dataloader) // pc.gradient_accumulation_steps
    max_train_steps = pc.epochs * num_update_steps_per_epoch
    warm_steps = int(pc.warmup_ratio * max_train_steps)
    
    print(f"ğŸ“ˆ è®­ç»ƒé…ç½®:")
    print(f"  - è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataloader.dataset)}")
    print(f"  - éªŒè¯æ ·æœ¬æ•°: {len(dev_dataloader.dataset)}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {pc.batch_size}")
    print(f"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {pc.gradient_accumulation_steps}")
    print(f"  - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {pc.batch_size * pc.gradient_accumulation_steps}")
    print(f"  - è®­ç»ƒè½®æ•°: {pc.epochs}")
    print(f"  - æ€»è®­ç»ƒæ­¥æ•°: {max_train_steps}")
    print(f"  - é¢„çƒ­æ­¥æ•°: {warm_steps}")
    
    # é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        name='linear',
        optimizer=optimizer,
        num_warmup_steps=warm_steps,
        num_training_steps=max_train_steps,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ“ å¼€å§‹è®­ç»ƒ...")
    loss_list = []
    tic_train = time.time()
    global_step, best_eval_loss = 0, float('inf')
    
    model.train()
    
    for epoch in range(1, pc.epochs + 1):
        print(f"\nğŸ”„ Epoch {epoch}/{pc.epochs}")
        print_gpu_memory()
        
        epoch_loss = []
        optimizer.zero_grad()
        
        for step, batch in enumerate(train_dataloader):
            # å‰å‘ä¼ æ’­
            if pc.fp16 and scaler is not None:
                with autocast_cuda():
                    loss = model(
                        input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),
                        labels=batch['labels'].to(dtype=torch.long, device=pc.device)
                    ).loss
                    # æ¢¯åº¦ç´¯ç§¯
                    loss = loss / pc.gradient_accumulation_steps
            else:
                loss = model(
                    input_ids=batch['input_ids'].to(dtype=torch.long, device=pc.device),
                    labels=batch['labels'].to(dtype=torch.long, device=pc.device)
                ).loss
                loss = loss / pc.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            if pc.fp16 and scaler is not None:
                scaler.scale(loss).backward()
            else:
                loss.backward()
            
            epoch_loss.append(float(loss.cpu().detach()) * pc.gradient_accumulation_steps)
            
            # æ¢¯åº¦ç´¯ç§¯å’Œä¼˜åŒ–å™¨æ­¥éª¤
            if (step + 1) % pc.gradient_accumulation_steps == 0:
                if pc.fp16 and scaler is not None:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                
                lr_scheduler.step()
                optimizer.zero_grad()
                global_step += 1
                
                # è®°å½•æ—¥å¿—
                if global_step % pc.logging_steps == 0:
                    time_diff = time.time() - tic_train
                    loss_avg = sum(epoch_loss[-pc.logging_steps*pc.gradient_accumulation_steps:]) / min(len(epoch_loss), pc.logging_steps*pc.gradient_accumulation_steps)
                    current_lr = lr_scheduler.get_last_lr()[0]
                    
                    print(f"ğŸ“Š Step {global_step}/{max_train_steps} ({global_step/max_train_steps*100:.1f}%) | "
                          f"Epoch {epoch} | Loss: {loss_avg:.5f} | LR: {current_lr:.2e} | "
                          f"Speed: {pc.logging_steps/time_diff:.2f} step/s")
                    print_gpu_memory()
                    
                    tic_train = time.time()
                
                # è¯„ä¼°å’Œä¿å­˜æ¨¡å‹
                if global_step % pc.eval_steps == 0:
                    print("ğŸ” å¼€å§‹è¯„ä¼°...")
                    eval_loss = evaluate_model(model, dev_dataloader)
                    print(f"ğŸ“ˆ Evaluation Loss: {eval_loss:.5f}")
                    
                    if eval_loss < best_eval_loss:
                        print(f"ğŸ‰ æœ€ä½³æ¨¡å‹æ›´æ–°: {best_eval_loss:.5f} â†’ {eval_loss:.5f}")
                        best_eval_loss = eval_loss
                        best_model_dir = os.path.join(pc.save_dir, "model_best")
                        save_model(model, best_model_dir)
                        tokenizer.save_pretrained(best_model_dir)
                        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_dir}")
                    
                    model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
                    clear_gpu_cache()
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % pc.save_freq == 0:
                    checkpoint_dir = os.path.join(pc.save_dir, f"checkpoint-{global_step}")
                    save_model(model, checkpoint_dir)
                    tokenizer.save_pretrained(checkpoint_dir)
                    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {checkpoint_dir}")
        
        # æ¯ä¸ªepochç»“æŸåçš„ç»Ÿè®¡
        epoch_avg_loss = sum(epoch_loss) / len(epoch_loss) if epoch_loss else 0
        print(f"âœ… Epoch {epoch} å®Œæˆ | å¹³å‡æŸå¤±: {epoch_avg_loss:.5f}")
        
        # æ¸…ç†æ˜¾å­˜
        clear_gpu_cache()
    
    print("ğŸŠ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_eval_loss:.5f}")
    print_gpu_memory()


if __name__ == '__main__':
    model2train()