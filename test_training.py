#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChatGLM-6B QLoRA ä¸‰å…ƒç»„æŠ½å–è®­ç»ƒæµ‹è¯•è„šæœ¬

ç”¨äºéªŒè¯è®­ç»ƒç¯å¢ƒå’Œä»£ç çš„ç¨³å®šæ€§
"""

import os
import sys
import torch
import traceback
from glm_config import ProjectConfig
from data_handle.data_loader import get_data
from data_handle.data_preprocess import validate_triplet_format

def test_environment():
    """æµ‹è¯•è¿è¡Œç¯å¢ƒ"""
    print("ğŸ” æµ‹è¯•è¿è¡Œç¯å¢ƒ...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # æ£€æŸ¥transformers
    try:
        import transformers
        print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformersæœªå®‰è£…")
        return False
    
    # æ£€æŸ¥peft
    try:
        import peft
        print(f"PEFTç‰ˆæœ¬: {peft.__version__}")
    except ImportError:
        print("âŒ PEFTæœªå®‰è£…")
        return False
    
    # æ£€æŸ¥datasets
    try:
        import datasets
        print(f"Datasetsç‰ˆæœ¬: {datasets.__version__}")
    except ImportError:
        print("âŒ Datasetsæœªå®‰è£…")
        return False
    
    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    return True

def test_config():
    """æµ‹è¯•é…ç½®"""
    print("\nğŸ” æµ‹è¯•é…ç½®...")
    
    try:
        pc = ProjectConfig()
        print(f"è®¾å¤‡: {pc.device}")
        print(f"é¢„è®­ç»ƒæ¨¡å‹: {pc.pre_model}")
        print(f"è®­ç»ƒæ•°æ®è·¯å¾„: {pc.train_path}")
        print(f"éªŒè¯æ•°æ®è·¯å¾„: {pc.dev_path}")
        print(f"è¾“å‡ºç›®å½•: {pc.save_dir}")
        print(f"æ‰¹æ¬¡å¤§å°: {pc.batch_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {pc.gradient_accumulation_steps}")
        print(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {pc.batch_size * pc.gradient_accumulation_steps}")
        print(f"åºåˆ—é•¿åº¦: {pc.max_source_seq_len} + {pc.max_target_seq_len} = {pc.max_seq_length}")
        print(f"LoRA rank: {pc.lora_rank}")
        print(f"æ··åˆç²¾åº¦: {pc.fp16}")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        if not os.path.exists(pc.train_path):
            print(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pc.train_path}")
            return False
        if not os.path.exists(pc.dev_path):
            print(f"âŒ éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pc.dev_path}")
            return False
        
        print("âœ… é…ç½®æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_data_format():
    """æµ‹è¯•æ•°æ®æ ¼å¼"""
    print("\nğŸ” æµ‹è¯•æ•°æ®æ ¼å¼...")
    
    try:
        pc = ProjectConfig()
        
        # è¯»å–å‡ ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
        import json
        with open(pc.train_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()[:5]  # åªæµ‹è¯•å‰5ä¸ªæ ·æœ¬
        
        valid_count = 0
        for i, line in enumerate(lines):
            try:
                data = json.loads(line.strip())
                context = data.get('context', '')
                target = data.get('target', '')
                
                print(f"æ ·æœ¬ {i+1}:")
                print(f"  è¾“å…¥é•¿åº¦: {len(context)}")
                print(f"  è¾“å‡ºé•¿åº¦: {len(target)}")
                
                # éªŒè¯ä¸‰å…ƒç»„æ ¼å¼
                if validate_triplet_format(target):
                    valid_count += 1
                    print(f"  æ ¼å¼: âœ… æœ‰æ•ˆ")
                else:
                    print(f"  æ ¼å¼: âŒ æ— æ•ˆ")
                    print(f"  ç›®æ ‡æ–‡æœ¬: {target[:100]}...")
                    
            except Exception as e:
                print(f"  è§£æå¤±è´¥: {e}")
        
        print(f"\næœ‰æ•ˆæ ·æœ¬: {valid_count}/{len(lines)}")
        
        if valid_count == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ä¸‰å…ƒç»„æ ·æœ¬")
            return False
        
        print("âœ… æ•°æ®æ ¼å¼æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ” æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        train_dataloader, dev_dataloader = get_data()
        
        print(f"è®­ç»ƒæ•°æ®æ‰¹æ¬¡æ•°: {len(train_dataloader)}")
        print(f"éªŒè¯æ•°æ®æ‰¹æ¬¡æ•°: {len(dev_dataloader)}")
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        for batch in train_dataloader:
            print(f"è¾“å…¥å½¢çŠ¶: {batch['input_ids'].shape}")
            print(f"æ ‡ç­¾å½¢çŠ¶: {batch['labels'].shape}")
            print(f"è¾“å…¥æ•°æ®ç±»å‹: {batch['input_ids'].dtype}")
            print(f"æ ‡ç­¾æ•°æ®ç±»å‹: {batch['labels'].dtype}")
            break
        
        print("âœ… æ•°æ®åŠ è½½æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æ£€æŸ¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from transformers import AutoTokenizer, AutoModel, AutoConfig
        import peft
        from utils.common_utils import CastOutputToFloat
        
        pc = ProjectConfig()
        
        print("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(pc.pre_model, trust_remote_code=True)
        print(f"è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        
        print("åŠ è½½æ¨¡å‹é…ç½®...")
        config = AutoConfig.from_pretrained(pc.pre_model, trust_remote_code=True)
        
        print("åŠ è½½æ¨¡å‹...")
        model = AutoModel.from_pretrained(
            pc.pre_model,
            config=config,
            trust_remote_code=True,
            torch_dtype=torch.float16 if pc.fp16 else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        print("é…ç½®LoRA...")
        model.lm_head = CastOutputToFloat(model.lm_head)
        peft_config = peft.LoraConfig(
            task_type=peft.TaskType.CAUSAL_LM,
            inference_mode=False,
            r=pc.lora_rank,
            lora_alpha=pc.lora_alpha,
            lora_dropout=pc.lora_dropout,
            target_modules=["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
        )
        model = peft.get_peft_model(model, peft_config)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(pc.device)
        
        # æ‰“å°å‚æ•°ç»Ÿè®¡
        if hasattr(model, 'print_trainable_parameters'):
            model.print_trainable_parameters()
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("æµ‹è¯•å‰å‘ä¼ æ’­...")
        model.train()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input_ids = torch.randint(0, len(tokenizer), (1, 100)).to(pc.device)
        test_labels = torch.randint(0, len(tokenizer), (1, 100)).to(pc.device)
        
        with torch.no_grad():
            outputs = model(input_ids=test_input_ids, labels=test_labels)
            print(f"æŸå¤±: {outputs.loss.item():.4f}")
        
        print("âœ… æ¨¡å‹åŠ è½½æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æ£€æŸ¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_memory_usage():
    """æµ‹è¯•æ˜¾å­˜ä½¿ç”¨"""
    print("\nğŸ” æµ‹è¯•æ˜¾å­˜ä½¿ç”¨...")
    
    if not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ˜¾å­˜æµ‹è¯•")
        return True
    
    try:
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        
        # è·å–åˆå§‹æ˜¾å­˜
        initial_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_memory:.2f}GB")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹çš„æ˜¾å­˜ä½¿ç”¨
        pc = ProjectConfig()
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        batch_size = pc.batch_size
        seq_length = pc.max_seq_length
        vocab_size = 65024  # ChatGLM-6Bè¯æ±‡è¡¨å¤§å°
        
        # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_length)).to(pc.device)
        labels = torch.randint(0, vocab_size, (batch_size, seq_length)).to(pc.device)
        
        current_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"åŠ è½½æ•°æ®åæ˜¾å­˜: {current_memory:.2f}GB")
        
        # æ¸…ç†
        del input_ids, labels
        torch.cuda.empty_cache()
        
        final_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"æ¸…ç†åæ˜¾å­˜: {final_memory:.2f}GB")
        
        # æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶…è¿‡20GB
        max_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPUæ€»æ˜¾å­˜: {max_memory:.2f}GB")
        
        if current_memory > 20:
            print(f"âš ï¸ æ˜¾å­˜ä½¿ç”¨å¯èƒ½è¶…è¿‡20GBé™åˆ¶")
        else:
            print("âœ… æ˜¾å­˜ä½¿ç”¨æ£€æŸ¥é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ˜¾å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ChatGLM-6B QLoRA ä¸‰å…ƒç»„æŠ½å–è®­ç»ƒæµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("ç¯å¢ƒæ£€æŸ¥", test_environment),
        ("é…ç½®æ£€æŸ¥", test_config),
        ("æ•°æ®æ ¼å¼æ£€æŸ¥", test_data_format),
        ("æ•°æ®åŠ è½½æ£€æŸ¥", test_data_loading),
        ("æ˜¾å­˜ä½¿ç”¨æ£€æŸ¥", test_memory_usage),
        # ("æ¨¡å‹åŠ è½½æ£€æŸ¥", test_model_loading),  # è¿™ä¸ªæµ‹è¯•æ¯”è¾ƒè€—æ—¶ï¼Œå¯é€‰
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} é€šè¿‡")
            else:
                print(f"âŒ {test_name} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} å¼‚å¸¸: {e}")
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)