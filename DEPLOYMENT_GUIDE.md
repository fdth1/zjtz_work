# ğŸš€ ChatGLM-6B QLoRA ä¸‰å…ƒç»„æŠ½å– - 20GB GPU éƒ¨ç½²æŒ‡å—

æœ¬æŒ‡å—ä¸“é—¨é’ˆå¯¹20GB GPUæ˜¾å­˜ç¯å¢ƒçš„éƒ¨ç½²å’Œä½¿ç”¨ã€‚

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

- **GPU**: 20GB+ æ˜¾å­˜ï¼ˆå¦‚RTX 3090, RTX 4090, A100ç­‰ï¼‰
- **ç³»ç»Ÿå†…å­˜**: 32GB+ æ¨è
- **Python**: 3.8+
- **CUDA**: 11.7+
- **å­˜å‚¨ç©ºé—´**: 50GB+ å¯ç”¨ç©ºé—´

## ğŸ”§ éƒ¨ç½²æ­¥éª¤

### 1. ç¡®è®¤æ¨¡å‹ä½ç½®

ä»£ç å·²é…ç½®ä¸ºä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š
```
/root/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B
```

**æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼š**
```bash
ls -la /root/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B/
```

å¦‚æœæ¨¡å‹ä¸åœ¨æ­¤è·¯å¾„ï¼Œè¯·ï¼š
1. ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„ï¼Œæˆ–
2. ä¿®æ”¹ `glm_config.py` ä¸­çš„ `self.pre_model` å‚æ•°

### 2. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/fdth1/zjtz_work.git
cd zjtz_work
git checkout chatglm-6b-qlora-optimization
```

### 3. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 4. éªŒè¯ç¯å¢ƒ

```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
python check_model_path.py

# è¿è¡ŒåŸºç¡€æµ‹è¯•
python test_basic.py
```

### 5. å¼€å§‹è®­ç»ƒ

```bash
# ä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰
chmod +x start_training.sh
./start_training.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
python train.py
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–é…ç½®

é¡¹ç›®å·²é’ˆå¯¹20GBæ˜¾å­˜è¿›è¡Œä¼˜åŒ–ï¼š

### å†…å­˜ä¼˜åŒ–è®¾ç½®
- **æ‰¹æ¬¡å¤§å°**: 1ï¼ˆæœ€å°åŒ–æ˜¾å­˜å ç”¨ï¼‰
- **æ¢¯åº¦ç´¯ç§¯**: 8æ­¥ï¼ˆä¿è¯è®­ç»ƒæ•ˆæœï¼‰
- **æœ‰æ•ˆæ‰¹æ¬¡å¤§å°**: 8
- **æ··åˆç²¾åº¦**: FP16ï¼ˆèŠ‚çœ50%æ˜¾å­˜ï¼‰
- **LoRA rank**: 8ï¼ˆå¹³è¡¡æ•ˆæœä¸æ˜¾å­˜ï¼‰

### QLoRAé…ç½®
```python
# LoRAé…ç½®
lora_rank = 8
lora_alpha = 32
lora_dropout = 0.1

# é‡åŒ–é…ç½®
load_in_4bit = True
bnb_4bit_compute_dtype = torch.float16
```

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### æ˜¾å­˜ç›‘æ§
```bash
# å®æ—¶ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# æŸ¥çœ‹è¯¦ç»†æ˜¾å­˜ä½¿ç”¨
nvidia-smi --query-gpu=memory.used,memory.total --format=csv -l 1
```

### è®­ç»ƒç›‘æ§
```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f training.log

# ç›‘æ§è®­ç»ƒè¿›åº¦
watch -n 1 'ls -la output/chatglm-6b-triplet-qlora/'
```

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### 1. æ˜¾å­˜ä¸è¶³ (CUDA Out of Memory)

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# åœ¨ glm_config.py ä¸­è¿›ä¸€æ­¥é™ä½æ‰¹æ¬¡å¤§å°
self.batch_size = 1
self.gradient_accumulation_steps = 16  # å¢åŠ æ¢¯åº¦ç´¯ç§¯

# æˆ–å¯ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–
self.dataloader_num_workers = 0  # å‡å°‘æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹
```

### 2. æ¨¡å‹åŠ è½½å¤±è´¥

**æ£€æŸ¥æ­¥éª¤ï¼š**
```bash
# 1. éªŒè¯æ¨¡å‹è·¯å¾„
python check_model_path.py

# 2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
ls -la /root/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B/

# 3. æµ‹è¯•æ¨¡å‹åŠ è½½
python -c "from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained('/root/.cache/modelscope/hub/models/ZhipuAI/ChatGLM-6B', trust_remote_code=True); print('æ¨¡å‹åŠ è½½æˆåŠŸ')"
```

### 3. è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–å»ºè®®ï¼š**
- ç¡®ä¿ä½¿ç”¨SSDå­˜å‚¨
- å¢åŠ ç³»ç»Ÿå†…å­˜
- æ£€æŸ¥CPUä½¿ç”¨ç‡
- ä¼˜åŒ–æ•°æ®é¢„å¤„ç†

## ğŸ“ˆ è®­ç»ƒæ•ˆæœè¯„ä¼°

### è®­ç»ƒè¿‡ç¨‹ç›‘æ§
```bash
# æŸ¥çœ‹æŸå¤±æ›²çº¿
python -c "
import json
with open('output/chatglm-6b-triplet-qlora/trainer_state.json', 'r') as f:
    state = json.load(f)
    for log in state['log_history'][-10:]:
        if 'train_loss' in log:
            print(f'Step {log[\"step\"]}: Loss = {log[\"train_loss\"]:.4f}')
"
```

### æ¨ç†æµ‹è¯•
```bash
# äº¤äº’å¼æµ‹è¯•
python inference_triplet.py --interactive

# æ‰¹é‡æµ‹è¯•
python inference_triplet.py --input_file test_data.txt --output_file results.json
```

## ğŸ¯ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 1. æ¨¡å‹ä¿å­˜
è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¿å­˜åœ¨ï¼š
```
output/chatglm-6b-triplet-qlora/model_best/
```

### 2. æ¨ç†æœåŠ¡
```bash
# å¯åŠ¨æ¨ç†æœåŠ¡
python inference_triplet.py --server --port 8000

# æµ‹è¯•API
curl -X POST http://localhost:8000/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "ã€Šå¨˜å®¶çš„æ•…äº‹ç¬¬äºŒéƒ¨ã€‹æ˜¯å¼ ç²æ‰§å¯¼çš„ç”µè§†å‰§ã€‚"}'
```

### 3. æ€§èƒ½åŸºå‡†
åœ¨20GB GPUç¯å¢ƒä¸‹çš„é¢„æœŸæ€§èƒ½ï¼š
- **è®­ç»ƒé€Ÿåº¦**: ~2-3 samples/second
- **æ¨ç†é€Ÿåº¦**: ~10-15 samples/second
- **æ˜¾å­˜å ç”¨**: ~18-19GBï¼ˆè®­ç»ƒæ—¶ï¼‰
- **æ˜¾å­˜å ç”¨**: ~8-10GBï¼ˆæ¨ç†æ—¶ï¼‰

## ğŸ“ æ—¥å¿—å’Œè°ƒè¯•

### é‡è¦æ—¥å¿—æ–‡ä»¶
- `training.log`: è®­ç»ƒæ—¥å¿—
- `output/chatglm-6b-triplet-qlora/trainer_state.json`: è®­ç»ƒçŠ¶æ€
- `output/chatglm-6b-triplet-qlora/training_args.bin`: è®­ç»ƒå‚æ•°

### è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export TRANSFORMERS_VERBOSITY=debug
python train.py

# å¯ç”¨CUDAè°ƒè¯•
export CUDA_LAUNCH_BLOCKING=1
python train.py
```

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

### æ›´æ–°ä»£ç 
```bash
git pull origin chatglm-6b-qlora-optimization
```

### æ¸…ç†ç¼“å­˜
```bash
# æ¸…ç†PyTorchç¼“å­˜
python -c "import torch; torch.cuda.empty_cache()"

# æ¸…ç†Transformersç¼“å­˜
rm -rf ~/.cache/huggingface/transformers/
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. GPUå‹å·å’Œæ˜¾å­˜å¤§å°
2. CUDAç‰ˆæœ¬ï¼š`nvcc --version`
3. PyTorchç‰ˆæœ¬ï¼š`python -c "import torch; print(torch.__version__)"`
4. é”™è¯¯æ—¥å¿—å’Œå †æ ˆè·Ÿè¸ª
5. ç³»ç»Ÿé…ç½®ï¼š`nvidia-smi`

---

**æ³¨æ„**: æœ¬é…ç½®å·²é’ˆå¯¹20GBæ˜¾å­˜ç¯å¢ƒä¼˜åŒ–ï¼Œå¦‚æœæ‚¨çš„GPUæ˜¾å­˜æ›´å¤§æˆ–æ›´å°ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå…¶ä»–å‚æ•°ã€‚